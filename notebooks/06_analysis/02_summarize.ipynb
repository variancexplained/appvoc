{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HVJ8q3cqvXH"
      },
      "source": [
        "# Apple App Store Review Summarization\n",
        "Text summarization is the process of condensing a piece of text while retaining its core information and meaning. It aims to generate a concise and coherent summary that captures the key points of the original text. There are generally two types of text summarization: extractive and abstractive.\n",
        "\n",
        "1. Extractive Summarization:\n",
        "   - Extractive summarization involves selecting and extracting important sentences or phrases directly from the original text to create a summary.\n",
        "   - It relies on identifying significant sentences based on criteria such as importance, relevance, and frequency of occurrence.\n",
        "   - Extractive summarization methods often use techniques like ranking sentences using statistical or heuristic approaches, and then selecting the top-ranked sentences for the summary.\n",
        "   - While extractive summarization is relatively straightforward and computationally efficient, it may result in less coherent summaries, as it does not generate new sentences but rather extracts existing ones.\n",
        "\n",
        "2. Abstractive Summarization:\n",
        "   - Abstractive summarization involves generating *new* sentences that convey the essence of the original text in a more condensed form.\n",
        "   - This method requires understanding the meaning of the text and rephrasing it in a way that captures the main ideas while potentially using different words or sentence structures.\n",
        "   - Abstractive summarization often utilizes advanced natural language processing techniques, such as deep learning models like Transformer-based architectures, which have shown promising results in generating human-like summaries.\n",
        "   - While abstractive summarization can produce more coherent and concise summaries compared to extractive methods, it also poses significant challenges, including maintaining coherence, preserving factual accuracy, and avoiding the generation of incorrect or misleading information.\n",
        "\n",
        "Challenges in Text Summarization:\n",
        "1. Semantic Understanding: Ensuring that the summarization algorithm accurately comprehends the meaning and context of the original text is crucial for generating informative summaries.\n",
        "2. Coherence and Fluency: Generating summaries that are both coherent and fluent poses a challenge, especially in abstractive summarization, where the algorithm needs to produce human-like language.\n",
        "3. Preserving Key Information: Summarization algorithms must effectively identify and retain the most relevant and important information from the original text while discarding redundant or trivial details.\n",
        "4. Handling Variability: Texts can vary widely in terms of length, style, and complexity, making it challenging to develop a one-size-fits-all summarization approach that performs well across different types of texts.\n",
        "5. Evaluation Metrics: Assessing the quality of summaries objectively is difficult, as there may be multiple valid ways to summarize a given text. Developing robust evaluation metrics that capture the essence, relevance, and readability of summaries remains an ongoing research challenge.\n",
        "\n",
        "In this notebook, we will use the pretrained [PEGASUS](https://huggingface.co/docs/transformers/main/model_doc/pegasus) model to summarize App Store reviews. The model will be trained on 792,259 book reviews."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers[sentencepiece] --upgrade --quiet\n",
        "!pip install datasets --upgrade --quiet\n",
        "!pip install pyarrow==11.0.0 --quiet\n",
        "!pip install nltk --quiet\n",
        "!pip install rouge_score --upgrade --quiet\n",
        "!pip install evaluate --upgrade --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GMPGjAULub1N",
        "outputId": "cfc7b8db-81e8-4738-cdfb-a4b476314842"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.3/38.3 MB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.9/170.9 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m51.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.5/239.5 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.3/124.3 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.6/301.6 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.9/34.9 MB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datasets 2.18.0 requires pyarrow>=12.0.0, but you have pyarrow 11.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vVXSl1fCqvXK"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "# import evaluate\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from datasets import load_dataset\n",
        "from google.colab import drive\n",
        "from transformers import TFAutoModelForSeq2SeqLM\n",
        "from huggingface_hub import notebook_login\n",
        "from transformers import DataCollatorForSeq2Seq\n",
        "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
        "from transformers import create_optimizer\n",
        "from transformers import pipeline\n",
        "from transformers.keras_callbacks import PushToHubCallback\n",
        "import tensorflow as tf\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download(\"punkt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preliminaries\n",
        "The book reviews are stored on Google Drive. Below, we mount the drive and provide the paths to the book review training, validation, and test sets. In addition, we log into the Hugging Face Hub."
      ],
      "metadata": {
        "id": "R5F9cI_7syLw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount(\"/content/drive\")\n",
        "fp_train = \"drive/My Drive/appvoc/reviews/books/books_train.pkl\"\n",
        "fp_val = \"drive/My Drive/appvoc/reviews/books/books_val.pkl\"\n",
        "fp_test = \"drive/My Drive/appvoc/reviews/books/books_test.pkl\"\n",
        "notebook_login()\n"
      ],
      "metadata": {
        "id": "2Tzelqu4tD8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## App Store Book Review Corpus"
      ],
      "metadata": {
        "id": "WssqVBbcRY0C"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIOdoo-PqvXO"
      },
      "source": [
        "### Create Huggingface Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mpL7YNpvqvXO"
      },
      "outputs": [],
      "source": [
        "data_files = {\"train\": fp_train, \"validation\": fp_val, \"test\": fp_test}\n",
        "reviews_dataset = load_dataset(\"pandas\", data_files=data_files)\n",
        "reviews_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lc-UX0yLqvXO"
      },
      "source": [
        "## Sample Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q3MUIvq8AsYs"
      },
      "outputs": [],
      "source": [
        "def show_samples(dataset, num_samples=3, seed=42):\n",
        "    sample = dataset[\"train\"].shuffle(seed=seed).select(range(num_samples))\n",
        "    for example in sample:\n",
        "        print(f\"\\n'>> Title: {example['title']}'\")\n",
        "        print(f\"'>> Review: {example['content']}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1NlPnJobqvXP"
      },
      "outputs": [],
      "source": [
        "show_samples(reviews_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSylrhnKqvXP"
      },
      "source": [
        "## Preprocess the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JYshmjDdqvXQ"
      },
      "outputs": [],
      "source": [
        "max_input_length = 512\n",
        "max_target_length = 64\n",
        "model_checkpoint = \"google/pegasus-xsum\"\n",
        "tokenizer = PegasusTokenizer.from_pretrained(model_checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_function(reviews):\n",
        "    model_inputs = tokenizer(\n",
        "        reviews[\"content\"],\n",
        "        max_length=max_input_length,\n",
        "        truncation=True,\n",
        "    )\n",
        "    labels = tokenizer(\n",
        "        reviews[\"title\"], max_length=max_target_length, truncation=True\n",
        "    )\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs"
      ],
      "metadata": {
        "id": "dxOkwd5x10oA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_reviews = reviews_dataset.map(preprocess_function, batched=True)"
      ],
      "metadata": {
        "id": "JKNyuwNy3O3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we remove the column names from the tokenized reviews."
      ],
      "metadata": {
        "id": "KazVAFhpgnba"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Baseline"
      ],
      "metadata": {
        "id": "XV6aTMex3iBK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def five_sentence_summary(text):\n",
        "  return \"\\n\".join(sent_tokenize(text)[:5])"
      ],
      "metadata": {
        "id": "zPWMdFkd3jzP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_baseline(dataset, metric):\n",
        "    summaries = [five_sentence_summary(text) for text in dataset[\"content\"]]\n",
        "    return metric.compute(predictions=summaries, references=dataset[\"title\"])"
      ],
      "metadata": {
        "id": "WUMsZ9mS4JAL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rouge_score = evaluate.load(\"rouge\")\n",
        "score = evaluate_baseline(reviews_dataset[\"validation\"], rouge_score)\n",
        "score"
      ],
      "metadata": {
        "id": "PYlbbM8Y4cGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-Tune Pegasus Model"
      ],
      "metadata": {
        "id": "53BWQjVE9wRe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = PegasusForConditionalGeneration.from_pretrained(model_checkpoint)"
      ],
      "metadata": {
        "id": "YY_bscZY8lds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Data Collator\n",
        "A data collator is an object that batches the data and, in some cases, performs some preprocessing. In this case, the Pegasus model is an encoder-decorder Transformer model; thus, we need to shift the labels to the right by one to ensure that the decoder only sees the previous ground truth labels and not the current or future labels. The DataCollatorForSeq2Seq collator will dynamically pad the inputs and labels accordingly."
      ],
      "metadata": {
        "id": "pRKDC52RE3We"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"tf\")"
      ],
      "metadata": {
        "id": "Gi7CSR4YE2FS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As the data collator will not know how to pad the column names, they must be removed from the tokenized reviews dataset."
      ],
      "metadata": {
        "id": "DAN_t-mCh4KI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_reviews = tokenized_reviews.remove_columns(\n",
        "    reviews_dataset[\"train\"].column_names\n",
        ")"
      ],
      "metadata": {
        "id": "k-AtpGo0gmEX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convert Datasets to TensorFlow Datasets\n",
        "Before we train the Pegasus model, we need to convert the tokenized reviews dataset to a tf.data.Datasets object using the data collator."
      ],
      "metadata": {
        "id": "Ms12WbjrfSkh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf_train_dataset = model.prepare_tf_dataset(\n",
        "    tokenized_reviews[\"train\"],\n",
        "    collate_fn=data_collator,\n",
        "    shuffle=True,\n",
        "    batch_size=8,\n",
        ")\n",
        "tf_val_dataset = model.prepare_tf_dataset(\n",
        "    tokenized_reviews[\"validation\"],\n",
        "    collate_fn=data_collator,\n",
        "    shuffle=False,\n",
        "    batch_size=8,\n",
        ")"
      ],
      "metadata": {
        "id": "iRhg3GFofajE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compile the Pegasus Model"
      ],
      "metadata": {
        "id": "SV5SCAeKibo6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The number of training steps is the number of samples in the dataset, divided by the batch size then multiplied\n",
        "# by the total number of epochs. Note that the tf_train_dataset here is a batched tf.data.Dataset,\n",
        "# not the original Hugging Face Dataset, so its len() is already num_samples // batch_size.\n",
        "num_train_epochs = 8\n",
        "num_train_steps = len(tf_train_dataset) * num_train_epochs\n",
        "model_name = model_checkpoint.split(\"/\")[-1]\n",
        "\n",
        "optimizer, schedule = create_optimizer(\n",
        "    init_lr=5.6e-5,\n",
        "    num_warmup_steps=0,\n",
        "    num_train_steps=num_train_steps,\n",
        "    weight_decay_rate=0.01,\n",
        ")\n",
        "\n",
        "model.compile(optimizer=optimizer)\n",
        "\n",
        "# Train in mixed-precision float16\n",
        "tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")"
      ],
      "metadata": {
        "id": "xbIKOLN2igsU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fit Pegasus Model\n",
        "Finally, we fit the model and use the PushToHUbCallback to save the model to the Hugging Face Hub after each epoch for inference later."
      ],
      "metadata": {
        "id": "N3m5-_jWjxVs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "callback = PushToHubCallback(\n",
        "    output_dir=f\"{model_name}-finetuned-appvoc_books-en\", tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "model.fit(\n",
        "    tf_train_dataset, validation_data=tf_val_dataset, callbacks=[callback], epochs=8\n",
        ")"
      ],
      "metadata": {
        "id": "vylZw_SHj1sB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate Pegasus Model Performance on Validation Set\n",
        "We are provided the loss values from training; however, we'd like to see the ROUGE metrics we computed earlier. To get those metrics, we'll need to generate outputs from the model and convert them to strings.\n",
        "\n",
        "Here, we'll build some lists of labels and predictions for the ROUGE metric to compare. We'll also  compile our generation code with XLA, TensorFlow's accelerated linear algebra compiler. XLA applies various optimizations to the model's computation graph, and results in significant improvements to speed and memory usage.\n",
        "\n",
        "XLA works best when there is little variation in our input shapes. To handle this, we'll  pad our inputs to multiples of 128, and make a new dataset with the padding collator. Then, we'll apply the @tf.function(jit_compile=True) decorator to our generation function, which marks the whole function for compilation with XLA.\n",
        "\n"
      ],
      "metadata": {
        "id": "2kFDk2M9kine"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "generation_data_collator = DataCollatorForSeq2Seq(\n",
        "    tokenizer, model=model, return_tensors=\"tf\", pad_to_multiple_of=128\n",
        ")\n",
        "\n",
        "tf_generate_dataset = model.prepare_tf_dataset(\n",
        "    tokenized_datasets[\"validation\"],\n",
        "    collate_fn=generation_data_collator,\n",
        "    shuffle=False,\n",
        "    batch_size=8,\n",
        "    drop_remainder=True,\n",
        ")\n",
        "\n",
        "\n",
        "@tf.function(jit_compile=True)\n",
        "def generate_with_xla(batch):\n",
        "    return model.generate(\n",
        "        input_ids=batch[\"input_ids\"],\n",
        "        attention_mask=batch[\"attention_mask\"],\n",
        "        max_new_tokens=32,\n",
        "    )\n",
        "\n",
        "\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "for batch, labels in tqdm(tf_generate_dataset):\n",
        "    predictions = generate_with_xla(batch)\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    labels = labels.numpy()\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "    decoded_preds = [\"\\n\".join(sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
        "    decoded_labels = [\"\\n\".join(sent_tokenize(label.strip())) for label in decoded_labels]\n",
        "    all_preds.extend(decoded_preds)\n",
        "    all_labels.extend(decoded_labels)"
      ],
      "metadata": {
        "id": "G3d8_qmAl47Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we compute the ROUGE scores."
      ],
      "metadata": {
        "id": "YkG6OgX7mUyC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = rouge_score.compute(\n",
        "    predictions=decoded_preds, references=decoded_labels, use_stemmer=True\n",
        ")\n",
        "result = {key: round(value.mid.fmeasure * 100,4) for key, value in result.items()}\n",
        "result_df = pd.DataFrame(result)\n",
        "result_df.head()"
      ],
      "metadata": {
        "id": "PbTD7i7_mX1O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate Fine-Tuned Model on Test Set"
      ],
      "metadata": {
        "id": "kTzuYWdsnZg-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hub_model_id = f\"j2slab/{model_name}-finetuned-appvoc_books-en\"\n",
        "summarizer = pipeline(\"summarization\", model=hub_model_id)"
      ],
      "metadata": {
        "id": "kRJVbZZloAsi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_summary(idx):\n",
        "    review = reviews_dataset[\"test\"][idx][\"content\"]\n",
        "    title = reviews_dataset[\"test\"][idx][\"title\"]\n",
        "    summary = summarizer(books_dataset[\"test\"][idx][\"content\"])[0][\"summary_text\"]\n",
        "    print(f\"'>>> Review: {review}'\")\n",
        "    print(f\"\\n'>>> Title: {title}'\")\n",
        "    print(f\"\\n'>>> Summary: {summary}'\")"
      ],
      "metadata": {
        "id": "upI77oy7n20I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_summary(100)"
      ],
      "metadata": {
        "id": "yXQv6vnEoF1P"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}